{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem need to be addressed \n",
    "\n",
    "In this research, we proposal a new fault tolerance analysis technique - fault tolerance boundary is designed specific to the data variables.\n",
    "\n",
    "why the huristic work, or why it does not work.\n",
    "\n",
    "\n",
    "# Can you think a neural network method to understand program's property?\n",
    "# Understand the propagation value of the nearest neighbor.\n",
    "# Sampling method\n",
    "\n",
    "\n",
    "\n",
    "1. What's the fault tolerance boundary.\n",
    "2. It's a technique can be equivlent to the fault injection campaign.\n",
    "3. We propose a method to approximate the boundary.\n",
    "    1. heuristic method to measure the boundary.\n",
    "    2. correlation analysis between variables to understand the relationship between the fault tolerance across variables.\n",
    "4. label propagation and active learning.\n",
    "5. The why masked prediction fail also can we use the SDC run to predict the program's outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "from IPython.core.debugger import set_trace\n",
    "from datetime import datetime\n",
    "import sys\n",
    "random.seed(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET = \"lu\" #current available datasets fft, lu, cg, other potential dataset.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhimin\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2698: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SDC ratio in this experiment is 0.31746922896182767\n",
      "The size of the experiment is 1000\n",
      "The size of the exhaust fault injection experiments 885248\n"
     ]
    }
   ],
   "source": [
    "# Threshold that used to decide whether the final outcome is SDC or Masked. For the loading data\n",
    "# set, there are two different datasets. One is the exhaust fault injection campaign which contains\n",
    "# all the fault injection information. The other is the test data set which is the sample experiment \n",
    "# contain the detail propagation information used for the downstream propagation analysis.\n",
    "PROPAGATION_DATA_PATH = \"\"\n",
    "CAMPAIGN_DATA_PATH = \"\"\n",
    "PROPAGATION_INJECTION_DATA_PATH = \"\"\n",
    "THRESHOLD = 0\n",
    "\n",
    "if DATASET == \"fft\":\n",
    "    PROPAGATION_DATA_PATH = \"../static/data/fft/fft_complete\"\n",
    "    CAMPAIGN_DATA_PATH = \"../static/data/fft/fft_injectlog.log\"\n",
    "    THRESHOLD = 0.001\n",
    "elif DATASET == \"cg\":\n",
    "    PROPAGATION_DATA_PATH = \"../static/data/cg/cg_in8\"\n",
    "    CAMPAIGN_DATA_PATH = \"../static/data/cg/cg_in8/injectlog.log\"\n",
    "    THRESHOLD = 0.06\n",
    "elif DATASET == \"lu\":\n",
    "    PROPAGATION_DATA_PATH = \"../static/data/lu/lu20000\"\n",
    "    CAMPAIGN_DATA_PATH = \"../static/data/lu/injectlog.log\"\n",
    "    THRESHOLD = 0.0001\n",
    "elif DATASET == \"bs\":\n",
    "    PROPAGATION_DATA_PATH = \"../static/data/bs/bs\"\n",
    "    CAMPAIGN_DATA_PATH = \"../static/data/bs/injectlog_complete.log\"\n",
    "    THRESHOLD = 0.0001\n",
    "     \n",
    "# The fault injection campaign experiment\n",
    "FAULT_INJECTION_CAMPAIGN = pd.read_csv(CAMPAIGN_DATA_PATH,  sep=\" \", names=['fileindex', 'file', 'linenum', 'variable','byte_num', 'corrupted','init_value','to', 'corrupt_value', 'mask', 'byte','expo', 'ss', 'op', 'diffnorm', 'empty'])\n",
    "\n",
    "# The size of the fault injection experiment\n",
    "SIZE = len(FAULT_INJECTION_CAMPAIGN)\n",
    "\n",
    "# The percentage of the test experiment over exhaust fault injection campaign.\n",
    "TEST_EXPERIMENT_NUMBER = int(SIZE * 0.01)\n",
    "\n",
    "# Golden Run\n",
    "GOLDEN_RUN = pd.read_csv(PROPAGATION_DATA_PATH+\"/golden.log\",  sep=\" \", names=['file', 'linenum', 'variable', 'value'])\n",
    "TEST_EXPERIMENTS = pd.read_csv(PROPAGATION_DATA_PATH+\"/injectlog.log\",  sep=\" \", names=['fileindex', 'file', 'linenum', 'variable','byte_num', 'corrupted','init_value','to', 'corrupt_value', 'mask', 'byte','expo', 'ss', 'op', 'diffnorm', 'empty'])\n",
    "\n",
    "if len(TEST_EXPERIMENTS) > 1000:\n",
    "    TEST_EXPERIMENTS = TEST_EXPERIMENTS.sample(n=1000, replace=True, random_state=1)\n",
    "\n",
    "# SDC ratio of test experiments\n",
    "SDC_COUNT = 0\n",
    "for index, row in FAULT_INJECTION_CAMPAIGN.iterrows():\n",
    "    if float(row['diffnorm']) > THRESHOLD:\n",
    "        SDC_COUNT += 1\n",
    "\n",
    "print(\"The SDC ratio in this experiment is\",SDC_COUNT/len(FAULT_INJECTION_CAMPAIGN))\n",
    "print(\"The size of the experiment is\",  len(TEST_EXPERIMENTS))\n",
    "print(\"The size of the exhaust fault injection experiments\", len(FAULT_INJECTION_CAMPAIGN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Golden Fault Injection Boundary\n",
    "Golden fault injection boundary use the exhaust fault inject campaign to build the boundary in practical sense. If the error value is above a specific threshold, then the location will not tolerance the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In fault injection campaign analysis, the prediction outcome is positive and negative.\n",
    "# during the prediction process, we ignore the crash case.\n",
    "# For the prediction, we need to think about four different cases\n",
    "# True negative, False positive\n",
    "# False negative, True positive\n",
    "def predict(boundary):\n",
    "    positive_prediction = 0\n",
    "    crash = 0\n",
    "    negative_prediction = 0\n",
    "    result = {}\n",
    "    bits = []\n",
    "    nan = float('nan')\n",
    "    inf = float('inf')\n",
    "    \n",
    "    mitigation_set = {}\n",
    "    count = 0\n",
    "\n",
    "    for index, row in FAULT_INJECTION_CAMPAIGN.iterrows():\n",
    "        instruction_index = int(row[\"byte_num\"].split(\"#\")[1])\n",
    "        inject_error = 0\n",
    "        corrupt_value = float(row['corrupt_value'])\n",
    "        init_value = float(row['init_value'])\n",
    "        \n",
    "        if init_value != 0 and corrupt_value != nan and corrupt_value != inf:\n",
    "            #inject_error = abs((corrupt_value - init_value)/init_value)\n",
    "            inject_error = abs(corrupt_value - init_value)\n",
    "        \n",
    "        diffnorm = float(row['diffnorm'])\n",
    "        \n",
    "        #if boundary[instruction_index] > 0 and inject_error >\n",
    "            \n",
    "        \n",
    "        if inject_error < boundary[instruction_index]:\n",
    "            #bits.append(row[\"bit\"])\n",
    "            if diffnorm < THRESHOLD:\n",
    "                positive_prediction += 1\n",
    "            elif math.isnan(diffnorm) or math.isinf(diffnorm):\n",
    "                crash += 1\n",
    "                #print(row)\n",
    "                #print(row['linenum'],row['variable'],row['corrupt_value'], row['init_value'])\n",
    "            else:\n",
    "                negative_prediction += 1 \n",
    "                #print(row)\n",
    "         \n",
    "        if inject_error < boundary[instruction_index] and diffnorm > 10:\n",
    "            key = str(row['linenum']) + row['variable']\n",
    "            print(row['fileindex'])\n",
    "            #if key in mitigation_set:\n",
    "            #    mitigation_set[key] += 1\n",
    "            #else:\n",
    "            #    mitigation_set[key] = 1          \n",
    "    \n",
    "  \n",
    "    result[\"positive\"] = positive_prediction\n",
    "    result[\"negative\"] = negative_prediction\n",
    "    result[\"crash\"] = crash\n",
    "    result[\"accuracy\"] = positive_prediction/(positive_prediction + negative_prediction)\n",
    "    \n",
    "    return (result, bits)\n",
    "\n",
    "def getGoldenFaultToleranceBoundary():\n",
    "    boundary = {}\n",
    "    nan = float('nan')\n",
    "    inf = float('inf')\n",
    "    for index, row in FAULT_INJECTION_CAMPAIGN.iterrows():\n",
    "        corrupt_value = float(row['corrupt_value'])\n",
    "        init_value = float(row['init_value'])\n",
    "        inject_error = 0\n",
    "        norm = float(row[\"diffnorm\"])\n",
    "        \n",
    "        try:\n",
    "            instruction_index = int(row[\"byte_num\"].split(\"#\")[1])\n",
    "        except:\n",
    "            print(row)\n",
    "        \n",
    "        if init_value != 0 and corrupt_value != nan and corrupt_value != inf:\n",
    "            inject_error = abs((corrupt_value - init_value)/init_value)\n",
    "  \n",
    "        if instruction_index not in boundary:\n",
    "            boundary[instruction_index] = float(\"+inf\")\n",
    "        \n",
    "        if norm > THRESHOLD:\n",
    "            if boundary[instruction_index] > inject_error:\n",
    "                boundary[instruction_index] = inject_error   \n",
    "        \n",
    "    return boundary\n",
    "\n",
    "#boundary = getGoldenFaultToleranceBoundary()\n",
    "#print(predict(boundary)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference Evaluation\n",
    "1. We uniformly random sample 1000 experiments and use the masked cases to predict the first 5, 10, 20, 50, 100, 200, 300 corrupted instructions' outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.9999928176398766\n",
      "0.3296191553544492\n",
      "10\n",
      "0.9999854763728465\n",
      "0.3253755778120188\n",
      "20\n",
      "0.9999799791647974\n",
      "0.319899644308943\n",
      "50\n",
      "0.9999833664348153\n",
      "0.3155008012820512\n",
      "100\n",
      "0.999983954317478\n",
      "0.3121044019507187\n",
      "200\n",
      "0.9999783741352042\n",
      "0.3156327110389611\n",
      "300\n",
      "0.9999926143025122\n",
      "0.31374087712314225\n"
     ]
    }
   ],
   "source": [
    "experiment_set=[5, 10, 20, 50, 100, 200, 300]\n",
    "l = len(GOLDEN_RUN)\n",
    "\n",
    "for knn in experiment_set:\n",
    "    \n",
    "    prediction_accuracy = []\n",
    "    prediction_amount = []\n",
    "    for index, row in TEST_EXPERIMENTS.iterrows():\n",
    "\n",
    "\n",
    "        #Ignore the SDC cases\n",
    "        if float(row[\"diffnorm\"]) > THRESHOLD:\n",
    "            continue\n",
    "\n",
    "        #\n",
    "        fault_inject_run = pd.read_csv(PROPAGATION_DATA_PATH+\"/appstate_\"+str(index)+\".log\",  sep=' ', names=['file', 'linenum', 'variable', 'value'])\n",
    "        if len(fault_inject_run) < l:\n",
    "            print(\"bad case!\")\n",
    "            continue\n",
    "        else:\n",
    "            fault_inject_run_value = np.array(fault_inject_run.value, dtype='float')[0:l] - np.array(GOLDEN_RUN.value, dtype='float')\n",
    "            fault_inject_run_value = np.abs(fault_inject_run_value)\n",
    "\n",
    "        prediction_index = np.where(fault_inject_run_value != 0)[0][1:1+knn]\n",
    "\n",
    "        if len(prediction_index) != knn:\n",
    "            continue\n",
    "\n",
    "        correct = 0\n",
    "        incorrect = 0\n",
    "        for i in prediction_index:\n",
    "            query_result = FAULT_INJECTION_CAMPAIGN[i*64:(i+1)*64]        \n",
    "\n",
    "            for index, row in query_result.iterrows():\n",
    "                inject_error = abs(float(row['init_value']) - float(row['corrupt_value']))\n",
    "                diff = float(row['diffnorm'])\n",
    "\n",
    "                if inject_error < fault_inject_run_value[i] and diff < THRESHOLD:\n",
    "                    correct+=1\n",
    "                elif inject_error < fault_inject_run_value[i] and diff >= THRESHOLD:\n",
    "                    incorrect += 1\n",
    "\n",
    "        if correct+incorrect == 0:\n",
    "            prediction_accuracy.append(1)\n",
    "            prediction_amount.append(0)\n",
    "        else:\n",
    "            prediction_accuracy.append(correct/(correct+incorrect))\n",
    "            prediction_amount.append((correct + incorrect)/(knn * 64))\n",
    "    print(knn)\n",
    "    print(sum(prediction_accuracy)/len(prediction_accuracy))\n",
    "    print(sum(prediction_amount)/len(prediction_amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9969519556322132\n",
      "0.40640998887033963\n"
     ]
    }
   ],
   "source": [
    "#fault_inject_run = pd.read_csv(PROPAGATION_DATA_PATH+\"/appstate_\"+str(0)+\".log\",  sep=' ', names=['file', 'linenum', 'variable', 'value'])\n",
    "#fault_inject_run_value = np.array(fault_inject_run.value, dtype='float')[0:l] - np.array(GOLDEN_RUN.value, dtype='float')\n",
    "#fault_inject_run_value = np.abs(fault_inject_run_value)\n",
    "\n",
    "#print(len(np.where(fault_inject_run_value != 0)[0]))\n",
    "#print(TEST_EXPERIMENTS)\n",
    "print(sum(prediction_accuracy)/len(prediction_accuracy))\n",
    "print(sum(prediction_amount)/len(prediction_amount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Mitigate and Amplify Errors\n",
    "\n",
    "1. How do you explain that case that some masked above the propagation boundary, some sdc case below the sdc cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#FAULT_INJECTION_CAMPAIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Single Fault Injection Experiment\n",
    "\n",
    "Understand how a single fault injection experiment can help to understand the result of the other fault injection experiment.\n",
    "\n",
    "Randomly select 1000 fault injection experiments and test the prediction accuracy of each masked experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The number of fault injection experiments that a single fault injection can predict is masked.\n",
    "def single_masked_prediction(fault_inject_run, golden_run, experiments, threshold = 0.001): \n",
    "    # The experiment ends early\n",
    "    if len(fault_inject_run) < len(golden_run):\n",
    "        print(\"Bad fault injection experiment!\")\n",
    "        return False\n",
    "    \n",
    "    # Get the prediction boundary of a single fault injection experiment.\n",
    "    boundary = np.abs(np.array(fault_inject_run.value[0:len(golden_run)], dtype=\"float\") - np.array(golden_run.value, dtype=\"float\"))    \n",
    "    if np.isnan(boundary).any() or np.isinf(boundary).any():\n",
    "        print(\"This experiment outcome is masked, but the propagation process contains nan or infinity event\")\n",
    "        return False\n",
    "    \n",
    "    # The number of tested experiment\n",
    "    # print(len(experiments))\n",
    "    \n",
    "    positive_prediction = 0\n",
    "    negative_prediction = 0\n",
    "    SDC_count = 0\n",
    "    crash = 0\n",
    "    count = len(experiments)\n",
    "  \n",
    "    for index, row in experiments.iterrows():\n",
    "        instruction_index = int(row[\"byte_num\"].split(\"#\")[1])\n",
    "        inject_error = abs(float(row[\"corrupt_value\"]) - float(row[\"init_value\"]))\n",
    "        diffnorm = float(row['diffnorm'])\n",
    "        \n",
    "        if inject_error <= boundary[instruction_index]:\n",
    "            if diffnorm < threshold:\n",
    "                positive_prediction += 1\n",
    "            elif math.isnan(diffnorm) or math.isinf(diffnorm):\n",
    "                crash += 1\n",
    "            else:\n",
    "                negative_prediction += 1     \n",
    "    \n",
    "    if (positive_prediction + negative_prediction) is 0:\n",
    "        return {\"crash\":crash/count, \"positive\": positive_prediction/count, \"negative\": negative_prediction/count, \"accuracy\":0,  \"total\":positive_prediction+negative_prediction}    \n",
    "    else:\n",
    "        return {\"crash\":crash/count, \"positive\": positive_prediction/count, \"negative\": negative_prediction/count, \"total\":positive_prediction+negative_prediction, \"accuracy\":positive_prediction/(positive_prediction + negative_prediction+crash)}  \n",
    "\n",
    "#The number of fault injection experiment that the program can predict as SDC\n",
    "def single_SDC_prediction(fault_inject_run, golden_run, experiments, threshold = 0.001):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# During the fault injection experiment, filter out the bad prediction case and left with the good prediction case.\n",
    "GOOD_MASKED_RUN = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diffnorms = np.array(FAULT_INJECTION_CAMPAIGN.diffnorm, dtype='float')\n",
    "length = len(TEST_EXPERIMENTS)\n",
    "list_indexs = []\n",
    "pre_res = {}\n",
    "index = 0\n",
    "\n",
    "sample_dataset = FAULT_INJECTION_CAMPAIGN.sample(n=int(TEST_EXPERIMENT_NUMBER/10), replace=False,  random_state=1)\n",
    "#Understand the pruning technique.\n",
    "print(len(sample_dataset))\n",
    "\n",
    "for i, row in TEST_EXPERIMENTS.iterrows():\n",
    "    file_index = row[\"fileindex\"]\n",
    "    norm = float(row[\"diffnorm\"])\n",
    "    \n",
    "    \n",
    "    if index > int(TEST_EXPERIMENT_NUMBER/10):\n",
    "        break\n",
    "    index += 1\n",
    "    if index % (length/10) ==0:\n",
    "        print(index/float(length), \"experiment\")\n",
    "    \n",
    "    #verify the select fault injection experiment is valuable\n",
    "    if norm != 0 and not np.isinf(norm) and not math.isnan(norm) and norm < THRESHOLD :\n",
    "        fault_inject_run = pd.read_csv(PROPAGATION_DATA_PATH+\"/appstate_\"+str(index)+\".log\",  sep=' ', names=['file', 'linenum', 'variable', 'value'])\n",
    "        pre_res = single_masked_prediction(fault_inject_run, GOLDEN_RUN, sample_dataset, THRESHOLD)\n",
    "        \n",
    "        #if the predictor reject the current experiment for prediction, continue to next experiment\n",
    "        if not pre_res:\n",
    "            continue\n",
    "        \n",
    "        if pre_res['accuracy'] >= 1 :\n",
    "            GOOD_MASKED_RUN.append(file_index)\n",
    "        \n",
    "        list_indexs.append({\"diffnorm\":math.log10(norm), \"index\":index, \"crash\":pre_res[\"crash\"], \"negative\":pre_res[\"negative\"], \"positive\":pre_res[\"positive\"], 'accuracy':pre_res['accuracy']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GOOD_MASKED_RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('seaborn-whitegrid')\n",
    "fig = plt.figure()\n",
    "list_indexs = sorted(list_indexs, key = lambda i: i['diffnorm']) \n",
    "x = []\n",
    "y = []\n",
    "z = []\n",
    "k = []\n",
    "for i in range(len(list_indexs)):\n",
    "    x.append(list_indexs[i]['diffnorm'])\n",
    "    y.append(list_indexs[i]['positive'] + list_indexs[i]['negative'] + list_indexs[i]['crash'])\n",
    "    z.append(list_indexs[i]['negative'])\n",
    "    k.append(list_indexs[i]['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\")\n",
    "sns.axes_style('white')\n",
    "df = pd.DataFrame(data = {\"x\":x, \"y\":y, \"k\":k,\"z\":z})\n",
    "\n",
    "plt.scatter(x, y, c=k, cmap='viridis')\n",
    "plt.xlabel('norm(log10)')\n",
    "plt.ylabel('prediction rate')\n",
    "#plt.clim(0,1)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(df.x)\n",
    "plt.xlabel('norm')\n",
    "plt.ylabel('number of experiments')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(df.y)\n",
    "plt.xlabel('positive prediction')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(df.k)\n",
    "plt.xlabel('prediction accuracy')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.hist(df.z)\n",
    "plt.xlabel('negative prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We use 1000 experiments as a filter to filter out outlier propagation data\n",
    "\n",
    "1. here is a very interesting, we use the subset of the sample to filter the outlier propagation data. Why this type of data is outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tracking_length = len(GOLDEN_RUN.value)\n",
    "def construct_boundary(cases):\n",
    "    boundary = []\n",
    "    for i in range(tracking_length):\n",
    "        boundary.append(0)\n",
    "\n",
    "    for i in range(len(cases)):\n",
    "        index = cases[i]\n",
    "        #masked_run_path = \"../static/data/cg/cg_in8/appstate_\"+ str(index)+\".log\" \n",
    "        masked_run_path = PROPAGATION_DATA_PATH +\"/appstate_\"+ str(index) + \".log\"\n",
    "        #Check whether the tracking file is on the path\n",
    "        #TODO: check whether can regenerate the data\n",
    "        #print(masked_run_path)\n",
    "        if not os.path.isfile(masked_run_path):\n",
    "            print(\"file does not exist\")\n",
    "            continue\n",
    "\n",
    "        masked_run = pd.read_csv(masked_run_path,  sep=\" \", names=['file', 'linenum', 'variable', 'value'])\n",
    "        masked_run_value = np.array(masked_run.value, dtype='float')\n",
    "        \n",
    "        #This is a werid information in the data.\n",
    "        #TODO: may check why such outcome is shown\n",
    "        if len(masked_run_value) < tracking_length:\n",
    "            print(\"odd!\")\n",
    "            continue\n",
    "\n",
    "        #array contain NAN, ignore the run\n",
    "        if np.isnan(np.min(masked_run_value)):\n",
    "            continue\n",
    "\n",
    "        comparision_result = np.abs(masked_run_value[0:tracking_length] - GOLDEN_RUN.value)\n",
    "        for j in range(tracking_length):\n",
    "            if comparision_result[j] > boundary[j]:\n",
    "                boundary[j] = comparision_result[j]\n",
    "                \n",
    "    return boundary\n",
    "\n",
    "def predict(boundary):\n",
    "    positive_prediction = 0\n",
    "    crash = 0\n",
    "    negative_prediction = 0\n",
    "    result = {}\n",
    "    bits = []\n",
    "\n",
    "    for index, row in FAULT_INJECTION_CAMPAIGN.iterrows():\n",
    "        instruction_index = int(row['byte_num'].split('#')[1])#int(row[\"DI\"]) \n",
    "        \n",
    "        inject_error = abs(float(row['init_value']) - float(row['corrupt_value']))#abs(row[\"out_xor\"])\n",
    "        diffnorm = float(row['diffnorm'])\n",
    "        \n",
    "        if inject_error <= boundary[instruction_index]:\n",
    "            if diffnorm < THRESHOLD:\n",
    "                positive_prediction += 1\n",
    "            elif math.isnan(diffnorm) or math.isinf(diffnorm):\n",
    "                crash += 1\n",
    "            else:\n",
    "                negative_prediction += 1     \n",
    "    \n",
    "    result[\"positive\"] = positive_prediction\n",
    "    result[\"negative\"] = negative_prediction\n",
    "    result[\"crash\"] = crash\n",
    "    result[\"accuracy\"] = positive_prediction/(positive_prediction + crash + negative_prediction)\n",
    "    \n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FAULT_INJECTION_CAMPAIGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import dill\n",
    "#dill.load_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print(GOOD_MASKED_RUN)\n",
    "#boundary = construct_boundary(GOOD_MASKED_RUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'boundary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-6aac1eae7bbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#res, bits = predict(boundary)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboundary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#plt.hist(bits)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'boundary' is not defined"
     ]
    }
   ],
   "source": [
    "#res, bits = predict(boundary)\n",
    "res = predict(boundary)\n",
    "\n",
    "#plt.hist(bits)\n",
    "print(res)\n",
    "#plt.hist(df.x)\n",
    "#plt.xlabel('norm')\n",
    "#plt.ylabel('number of experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot the boundary\n",
    "#plt.plot(boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Extracting A single fault injection site to understand the error propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
